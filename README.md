# 2-3WeeksOrBust
Greek/English focused embedding models based on state of the art techniques, comparisons and evaluations

# Models
1) Meltemi-based LLM2Vec embedding model
2) NV-embed finetuning
3) Maybe try and finetune meltemi with the NV-embed method (need to change the pooling method with latent attention pooling)

# Unsupervised Datasets
1) Greek Wikipedia
2) Other high quality Greek Datasets
3) English Wikipedia

# Supervised Datasets
1) Greek NLI tasks
2) English NLI tasks

# Evaluations
1) MTEB (for English)
2) Pending (for Greek)
